{
  "cells": [https://github.com/Nagoudi/Perplexity/blob/main/Perplexity_Tutorial.ipynb
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki559_s5ap7z"
      },
      "source": [
        "# **Calculating perplexity with GPT-2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eppoA_piap71"
      },
      "source": [
        "## **üìö Definition:**\n",
        "\n",
        "Perplexity (PPL) is a widely used metric that estimates how well an autoregressive language model predicts a given text. The Perplexity is defined as the exponentiated average negative log-likelihood of a sequence. \n",
        "## **üî¢ Formula:**\n",
        "\n",
        "If we have a tokenized sequence $X = (x_0, x_1, \\dots, x_t)$, then the perplexity of $X$ is:\n",
        "\n",
        "$$\\text{PPL}(X) = \\exp \\left\\{ {-\\frac{1}{t}\\sum_i^t \\log p_\\theta (x_i|x_{<i}) } \\right\\}$$\n",
        "\n",
        "where $\\log p_\\theta (x_i|x_{<i})$ is the log-likelihood of the $i^{th}$ token conditioned on the preceding tokens $x_{<i}$ according to our model. \n",
        "\n",
        "\n",
        "## **üí° Note:**\n",
        "\n",
        "The tokenization procedure has a direct impact on a model's perplexity which should always be taken into consideration when comparing different models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77Ofxz8Bap73"
      },
      "source": [
        "## **Calculating PPL with fixed-length models**\n",
        "\n",
        "# Unlimited Context Size\n",
        "\n",
        "If we weren't limited by a model's context size, we would evaluate the model's perplexity by autoregressively\n",
        "factorizing a sequence and conditioning on the entire preceding subsequence at each step, as shown below.\n",
        "\n",
        "<img width=\"600\" alt=\"Full decomposition of a sequence with unlimited context length\" src=\"https://raw.githubusercontent.com/Nagoudi/Perplexity/main/ppl.gif\n",
        "\"/>\n",
        "\n",
        "\n",
        "# Fixed Context Size\n",
        "\n",
        "When working with autoregressive language model, we typically have a constraint on the number of tokens the model can process. [GPT-2](hhttps://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe), for example, has a fixed length of 1024 tokens, so we cannot calculate $p_\\theta(x_t|x_{<t})$ directly when $t$ is greater than 1024.\n",
        "\n",
        "Instead, the sequence is typically broken into subsequences equal to the model's maximum input size . If a model's max input size is $k$ (e.g., $k$=1024 for GPT2 and $k$= 2048 for GPT3), we then approximate the likelihood of a token $x_t$ by conditioning only on the $k-1$ tokens that precede it rather than the entire context. When evaluating the model's perplexity of a\n",
        "sequence, a tempting but suboptimal approach is to break the sequence into disjoint chunks and add up the decomposed\n",
        "log-likelihoods of each segment independently. The example below shows the previous example with a fixed context size of $k=5$.\n",
        "\n",
        "<img width=\"600\" alt=\"Suboptimal PPL not taking advantage of full available context\" src=\"https://raw.githubusercontent.com/Nagoudi/Perplexity/main/ppl2.gif\"/>\n",
        "\n",
        "\n",
        "# Sliding-Window Strategy\n",
        "\n",
        "This is quick to compute since the perplexity of each segment can be computed in one forward pass, but serves as a poor\n",
        "approximation of the fully-factorized perplexity and will typically yield a higher (worse) PPL because the model will\n",
        "have less context at most of the prediction steps.\n",
        "\n",
        "Instead, the PPL of fixed-length models should be evaluated with a sliding-window strategy. This involves repeatedly\n",
        "sliding the context window so that the model has more context when making each prediction. The example below  uses sliding-window size of $k=5$.\n",
        "\n",
        "<img width=\"600\" alt=\"Sliding window PPL taking advantage of all available context\" src=\"https://raw.githubusercontent.com/Nagoudi/Perplexity/main/ppl3.gif\"/>\n",
        "\n",
        "This is a closer approximation to the true decomposition of the sequence probability and will typically yield a more\n",
        "favorable score. The downside is that it requires a separate forward pass for each token in the corpus. A good\n",
        "practical compromise is to employ a strided sliding window, moving the context by larger strides rather than sliding by\n",
        "1 token a time. This allows computation to proceed much faster while still giving the model a large context to make\n",
        "predictions at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXMe3nIzap73"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgJJledfap74"
      },
      "source": [
        "# Example: Calculating perplexity with GPT-2 "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's first install Transformers from HuggingFace ü§ó"
      ],
      "metadata": {
        "id": "IWrjWl2RwGlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformers and datasets installation\n",
        "! pip install transformers datasets"
      ],
      "metadata": {
        "id": "IGPw_hBxuPKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the GPT-2's model and Toknizer"
      ],
      "metadata": {
        "id": "oH1XLv6jNWVm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVfvXnhCap76"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "\n",
        "device = \"cuda\"\n",
        "model_id = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and toknize WikiText-2 dataset "
      ],
      "metadata": {
        "id": "COs3ywloNeka"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOoZEG_wap77"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
        "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\") # Since this dataset is small and we're just doing one forward pass over the set, we can just load and encode the entire dataset in memory."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the perplexity using the sliding-window strategie\n",
        "\n",
        "With ü§ó Transformers, we can simply:\n",
        " \n",
        "\n",
        "1.   Pass the `input_ids` as the `labels` to our model\n",
        "2.   Average negative log-likelihood for each token is returned as the loss.\n",
        "\n",
        "\n",
        "## **üí° Note 1:**\n",
        "\n",
        "With our sliding window approach, however, there is overlap in the tokens we pass to the model at each iteration. We don't want the log-likelihood for the tokens we're just treating as context to be included in our loss, so we can set these targets to `-100` so that they are ignored. \n",
        "\n",
        "## **üìù Calculating perplexity:** \n",
        "\n",
        "The following is an example of how we could do this with a stride of `512`. This means that the model will have at least 512 tokens\n",
        "for context when calculating the conditional likelihood of any one token (provided there are 512 preceding tokens\n",
        "available to condition on).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WS-B2AT6OXVu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d-PvsBEap78"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "max_length = model.config.n_positions # For GPT-2 the max_length is 1024\n",
        "stride = 512  # we will use 512 tokens as sliding-window size \n",
        "seq_len = encodings.input_ids.size(1) # seq_len of the WikiText-2. \n",
        "\n",
        "nlls = []\n",
        "prev_end_loc = 0\n",
        "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
        "    end_loc = min(begin_loc + max_length, seq_len)\n",
        "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
        "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
        "    target_ids = input_ids.clone()\n",
        "    target_ids[:, :-trg_len] = -100\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=target_ids)\n",
        "        # loss is calculated using CrossEntropyLoss which averages over input tokens.\n",
        "        # Multiply it with trg_len to get the summation instead of average.\n",
        "        # We will take average over all the tokens to get the true average\n",
        "        # in the last step of this example.\n",
        "        neg_log_likelihood = outputs.loss * trg_len\n",
        "\n",
        "    nlls.append(neg_log_likelihood)\n",
        "\n",
        "    prev_end_loc = end_loc\n",
        "    if end_loc == seq_len:\n",
        "        break\n",
        "\n",
        "ppl = torch.exp(torch.stack(nlls).sum() / end_loc)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('The perplexity of GPT-2 on WikiText-2 is:', ppl.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0HHItdFYadj",
        "outputId": "54d8ae5e-2dd4-4c3c-fffa-0cbac4562ed4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The perplexity of GPT-2 on WikiText-2 is: 25.170446395874023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Source\n",
        "\n",
        "[1]: [GPT-2 Paper](https://www.semanticscholar.org/paper-Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)\n",
        "\n",
        "[2]  https://huggingface.co/docs/transformers\n",
        "\n",
        "[3]  https://huggingface.co/docs/transformers/perplexity."
      ],
      "metadata": {
        "id": "2xN3lIa40QTl"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
